{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "def split_dataset(df):\n",
    "    train, test, ytrain, ytest = train_test_split(df['item_name'], df['category'], test_size=0.2, random_state=88)\n",
    "\n",
    "    return train, test, ytrain, ytest\n",
    "\n",
    "\n",
    "def create_labels(train_labels, test_labels, labels):\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    encoder.fit(labels)\n",
    "\n",
    "    y_train = encoder.transform(train_labels)\n",
    "    y_val = encoder.transform(test_labels)\n",
    "\n",
    "    return y_train, y_val\n",
    "    \n",
    "\n",
    "def create_one_hot_labels(Y_train, Y_test, labels):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels)\n",
    "\n",
    "    Y_train = encoder.transform(Y_train)\n",
    "    Y_test = encoder.transform(Y_test)\n",
    "\n",
    "    n_classes = np.max(Y_train) + 1\n",
    "\n",
    "    Y_train = np_utils.to_categorical(Y_train, n_classes)\n",
    "    Y_test = np_utils.to_categorical(Y_test, n_classes)\n",
    "\n",
    "    return Y_train, Y_test, n_classes\n",
    "\n",
    "\n",
    "def transform_data(train, val):\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train) # BoW\n",
    "    X_train = transformer.fit_transform(X_train) # TF-IDF\n",
    "\n",
    "    X_val = vectorizer.transform(val)\n",
    "    X_val = transformer.transform(X_val)\n",
    "\n",
    "    print(X_train.shape)\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def get_nb_model(x,y):\n",
    "    nb = MultinomialNB().fit(x, y)\n",
    "    return nb\n",
    "\n",
    "\n",
    "def get_svm_model(x,y):\n",
    "    svm = LinearSVC(max_iter=1000).fit(x,y)\n",
    "    return svm\n",
    "\n",
    "\n",
    "def get_acc(m, x, y):\n",
    "    predictions = m.predict(x)\n",
    "    acc = np.mean(predictions == y)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_data = pd.read_csv(\"../cleaned/amazon-cleaned.csv\",index_col=0)\n",
    "amazon_labels = amazon_data.category.unique()\n",
    "\n",
    "shopmania_data = pd.read_csv(\"../cleaned/shopmania-cleaned.csv\", index_col=0)\n",
    "shopmania_labels = shopmania_data.category.unique()\n",
    "\n",
    "custom_data = pd.read_csv(\"../cleaned/custom-cleaned.csv\", names=[\"store_name\", \"item_name\", \"category\"])\n",
    "custom_labels = custom_data.category.unique()\n",
    "\n",
    "datasets = {'amazon': [amazon_data.copy(), amazon_labels], 'shopmania': [shopmania_data.copy(), shopmania_labels], 'custom': [custom_data.copy(), custom_labels]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopmania_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models, experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(df, name, labels):\n",
    "\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "\n",
    "    X_train, X_test = transform_data(X_train, X_test)\n",
    "\n",
    "    nb_model = get_nb_model(X_train, Y_train)\n",
    "\n",
    "    nb_acc = get_acc(nb_model, X_test, Y_test)\n",
    "    print(f\"Naive Bayes accuracy on {name} dataset: {nb_acc:.2f}%\")\n",
    "    \n",
    "    return nb_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    print()\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_nb(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(df, name, labels):\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "\n",
    "    X_train, X_test = transform_data(X_train, X_test)\n",
    "\n",
    "    svm_model = get_svm_model(X_train, Y_train)\n",
    "\n",
    "    svm_acc = get_acc(svm_model, X_test, Y_test)\n",
    "    print(f\"SVM accuracy on {name} dataset: {svm_acc:.2f}%\")\n",
    "    \n",
    "    return svm_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    print()\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    svm_acc = run_svm(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import keras.layers as l\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_nn_model(n_classes):\n",
    "    m = Sequential()\n",
    "\n",
    "    m.add(l.Dense(128, activation='relu'))\n",
    "    m.add(l.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    optim = tf.keras.optimizers.Adam()\n",
    "\n",
    "    m.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=optim, metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(history, name):\n",
    "    metrics = [[name, history.history['val_accuracy'][-1], history.history['val_loss'][-1]]]\n",
    "    \n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(df, name, labels):\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test, n_classes = create_one_hot_labels(Y_train, Y_test, labels)\n",
    "\n",
    "    X_train, X_test = transform_data(X_train, X_test)\n",
    "\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "\n",
    "    nn_model = get_nn_model(n_classes)\n",
    "\n",
    "    history = nn_model.fit(X_train, Y_train, batch_size=2, epochs=1, validation_data=(X_test, Y_test))\n",
    "    if name == 'amazon':\n",
    "        model_graph = nn_model\n",
    "\n",
    "    tf.keras.utils.plot_model(nn_model, to_file=name+'.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    visualizer(nn_model, format='png', view=True)\n",
    "    metrics = get_metrics(history, name)\n",
    "    print(f\"NN accuracy on {name} dataset: {metrics:.2f}%\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 5000)\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 1.8924 - accuracy: 0.6898 - val_loss: 0.6496 - val_accuracy: 0.8520\n",
      "NN accuracy on amazon dataset: 0.85%\n",
      "Time taken to process amazon dataset: 0.15m\n",
      "(8966, 5000)\n",
      "4483/4483 [==============================] - 6s 1ms/step - loss: 2.9162 - accuracy: 0.5157 - val_loss: 1.4957 - val_accuracy: 0.7230\n",
      "NN accuracy on shopmania dataset: 0.72%\n",
      "Time taken to process shopmania dataset: 0.16m\n",
      "(400, 396)\n",
      "200/200 [==============================] - 1s 2ms/step - loss: 1.9858 - accuracy: 0.5250 - val_loss: 1.3249 - val_accuracy: 0.5545\n",
      "NN accuracy on custom dataset: 0.55%\n",
      "Time taken to process custom dataset: 0.01m\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    nn_acc = run_nn(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(nn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Keras Visualizer: Error while visualizing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\keras_visualizer\\__init__.py:230\u001b[0m, in \u001b[0;36mvisualizer\u001b[1;34m(model, filename, format, view)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=228'>229</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=229'>230</a>\u001b[0m     graph\u001b[39m.\u001b[39mrender(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m, view\u001b[39m=\u001b[39mview)\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=230'>231</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'graph' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\University\\FYP\\final\\models.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/University/FYP/final/models.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_visualizer\u001b[39;00m \u001b[39mimport\u001b[39;00m visualizer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/University/FYP/final/models.ipynb#ch0000026?line=2'>3</a>\u001b[0m visualizer(model_graph, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m'\u001b[39;49m, view\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\keras_visualizer\\__init__.py:234\u001b[0m, in \u001b[0;36mvisualizer\u001b[1;34m(model, filename, format, view)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=231'>232</a>\u001b[0m         graph\u001b[39m.\u001b[39msave()\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=232'>233</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/keras_visualizer/__init__.py?line=233'>234</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mKeras Visualizer: Error while visualizing\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Keras Visualizer: Error while visualizing"
     ]
    }
   ],
   "source": [
    "from keras_visualizer import visualizer\n",
    "\n",
    "visualizer(model_graph, format='png', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({}, columns=[\"name\", \"nb\", \"svm\", \"nn\"])\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "    combined = list([key] + dataset[2:])\n",
    "    print(combined)\n",
    "    new_line = pd.DataFrame([combined], columns=[\"name\", \"nb\", \"svm\", \"nn\"])\n",
    "\n",
    "    results = pd.concat([results, new_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack(n):\n",
    "    n = n*100\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nn'] = results['nn'].apply(hack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.insert(1, 'lr', [85.85, 74.933095, 67.326733])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"classic_models.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f206b6c75a13f92bc97dfa1e930151b21df17184be9d9c72a2ae387f01f205b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('FYP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
