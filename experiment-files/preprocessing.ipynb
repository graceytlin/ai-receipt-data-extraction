{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lower(text):\n",
    "    result = \" \".join([word.lower() for word in text.split()])\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    remove = str.maketrans('', '', (string.punctuation + 'Â£'))\n",
    "    return text.translate(remove)\n",
    "\n",
    "\n",
    "def remove_nums(text):\n",
    "    remove = str.maketrans('', '', string.digits)\n",
    "    return text.translate(remove)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = \" \".join([word for word in text.split() if word not in stoplist])\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemmer_nltk(text):\n",
    "    stemmed = \" \".join(stemmer.stem(word) for word in text.split())\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def lemmatizer_nltk(text):\n",
    "    lemmatized = \" \".join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_dataset(df):\n",
    "    train, test, ytrain, ytest = train_test_split(df['item_name'], df['category'], test_size=0.2, random_state=88)\n",
    "\n",
    "    return train, test, ytrain, ytest\n",
    "\n",
    "\n",
    "def create_labels(train_labels, test_labels, labels):\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    encoder.fit(labels)\n",
    "\n",
    "    y_train = encoder.transform(train_labels)\n",
    "    y_val = encoder.transform(test_labels)\n",
    "\n",
    "    return y_train, y_val\n",
    "\n",
    "\n",
    "def get_vectors(train_data, val_data):\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    train_data = vectorizer.fit_transform(train_data)\n",
    "    val_data = vectorizer.transform(val_data)\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def get_lr_model(x, y, iter=100):\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=iter, n_jobs=8).fit(x, y)\n",
    "\n",
    "    return lr\n",
    "\n",
    "def get_nb_model(x,y):\n",
    "    nb = MultinomialNB().fit(x, y)\n",
    "\n",
    "    return nb\n",
    "\n",
    "def get_svm_model(x,y):\n",
    "    svm = SVC().fit(x,y)\n",
    "\n",
    "    return svm\n",
    "\n",
    "\n",
    "def get_acc(m, x, y):\n",
    "    predictions = m.predict(x)\n",
    "\n",
    "    acc = np.mean(predictions == y)*100\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_data = pd.read_csv(\"amazon-pqa-reduced-100.csv\",index_col=0)\n",
    "amazon_labels = amazon_data.category.unique()\n",
    "shopmania_data = pd.read_csv(\"shopmania-reduced-100.csv\", index_col=0)\n",
    "shopmania_labels = shopmania_data.category.unique()\n",
    "custom_data = pd.read_csv(\"products_list_final.csv\", index_col=0)\n",
    "custom_labels = custom_data.category.unique()\n",
    "custom_data.columns = [\"store_name\", \"item_name\", \"category\"]\n",
    "\n",
    "datasets = {'amazon': [amazon_data.copy(), amazon_labels], 'shopmania': [shopmania_data.copy(), shopmania_labels], 'custom': [custom_data.copy(), custom_labels]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>item_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>TESCO</td>\n",
       "      <td>CKN NUGGETS</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>TESCO</td>\n",
       "      <td>SCRATCHINGS</td>\n",
       "      <td>snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>TESCO</td>\n",
       "      <td>SOFT CHEESE</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>lidl</td>\n",
       "      <td>Green Tee Pomegrana</td>\n",
       "      <td>drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>LiDL</td>\n",
       "      <td>Tenderstem Broccoli</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_name            item_name   category\n",
       "574      TESCO          CKN NUGGETS  groceries\n",
       "557      TESCO          SCRATCHINGS     snacks\n",
       "56       TESCO          SOFT CHEESE  groceries\n",
       "380       lidl  Green Tee Pomegrana     drinks\n",
       "496       LiDL  Tenderstem Broccoli  groceries"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_data.sample(5, random_state=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>MSI AMD Radeon R9 290 4GB GDDR5 2DVI/HDMI/Disp...</td>\n",
       "      <td>graphics_cards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>Silverstone Technology CS380B Silverstone DIY ...</td>\n",
       "      <td>computer_cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>Baja Designs Ford F150 2017 Raptor S2 Reverse ...</td>\n",
       "      <td>light_bars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              item_name        category\n",
       "3452  MSI AMD Radeon R9 290 4GB GDDR5 2DVI/HDMI/Disp...  graphics_cards\n",
       "2121  Silverstone Technology CS380B Silverstone DIY ...  computer_cases\n",
       "5444  Baja Designs Ford F150 2017 Raptor S2 Reverse ...      light_bars"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>ncaa lightweight water resistant economy canop...</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>plum organics baby food organic pumpkin and ba...</td>\n",
       "      <td>Feeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>douglas contour 4 .30 1 10 twist ss 4 contour ...</td>\n",
       "      <td>Digital Camera and Camcorder Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              item_name  \\\n",
       "928   ncaa lightweight water resistant economy canop...   \n",
       "781   plum organics baby food organic pumpkin and ba...   \n",
       "4487  douglas contour 4 .30 1 10 twist ss 4 contour ...   \n",
       "\n",
       "                                      category  \n",
       "928                                       Toys  \n",
       "781                                    Feeding  \n",
       "4487  Digital Camera and Camcorder Accessories  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shopmania_data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_name</th>\n",
       "      <th>item_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>TESCO</td>\n",
       "      <td>CAFFE LATTE</td>\n",
       "      <td>drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TESCO</td>\n",
       "      <td>KITCHEN TOWELS</td>\n",
       "      <td>cleaning &amp; laundry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>T.K.maxx</td>\n",
       "      <td>ACCESSORIES &amp; LIFESTYLE</td>\n",
       "      <td>clothes &amp; accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_name                item_name               category\n",
       "235      TESCO              CAFFE LATTE                 drinks\n",
       "7        TESCO           KITCHEN TOWELS     cleaning & laundry\n",
       "110   T.K.maxx  ACCESSORIES & LIFESTYLE  clothes & accessories"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing, experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_remove_punctuation(df, name, labels):\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "    print(\"Checkpoint: split dataset\")\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    print(\"Checkpoint: encoded labels\")\n",
    "\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    print(\"Checkpoint: vectorised inputs\")\n",
    "\n",
    "    print(\"Checkpoint: starting LR\")\n",
    "    lr_model = get_lr_model(X_train, Y_train)\n",
    "\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "    \n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: split dataset\n",
      "Checkpoint: encoded labels\n",
      "Checkpoint: vectorised inputs\n",
      "Checkpoint: starting LR\n",
      "Logistic regression accuracy on amazon dataset: 84.20%\n",
      "Time taken to process amazon dataset: 0.00m\n",
      "Checkpoint: split dataset\n",
      "Checkpoint: encoded labels\n",
      "Checkpoint: vectorised inputs\n",
      "Checkpoint: starting LR\n",
      "Logistic regression accuracy on shopmania dataset: 78.19%\n",
      "Time taken to process shopmania dataset: 0.00m\n",
      "Checkpoint: split dataset\n",
      "Checkpoint: encoded labels\n",
      "Checkpoint: vectorised inputs\n",
      "Checkpoint: starting LR\n",
      "Logistic regression accuracy on custom dataset: 79.55%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_remove_punctuation(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_data():\n",
    "    datasets['amazon'][0] = amazon_data.copy()\n",
    "    datasets['shopmania'][0] = shopmania_data.copy()\n",
    "    datasets['custom'][0] = custom_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_remove_numbers(df, name, labels):\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(remove_nums)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    lr_model = get_lr_model(X_train, Y_train)\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "\n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on amazon dataset: 84.40%\n",
      "Time taken to process amazon dataset: 0.00m\n",
      "Logistic regression accuracy on shopmania dataset: 78.68%\n",
      "Time taken to process shopmania dataset: 0.00m\n",
      "Logistic regression accuracy on custom dataset: 81.06%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "# Reset dataset to compare\n",
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_remove_numbers(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stemmer(df, name, labels):\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(stemmer_nltk)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    lr_model = get_lr_model(X_train, Y_train)\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "\n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on amazon dataset: 84.65%\n",
      "Time taken to process amazon dataset: 0.04m\n",
      "Logistic regression accuracy on shopmania dataset: 79.13%\n",
      "Time taken to process shopmania dataset: 0.03m\n",
      "Logistic regression accuracy on custom dataset: 78.03%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "# Reset dataset to compare\n",
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_stemmer(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lemmatizer(df, name, labels):\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(lemmatizer_nltk)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    lr_model = get_lr_model(X_train, Y_train,5)\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "\n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on amazon dataset: 83.25%\n",
      "Time taken to process amazon dataset: 0.03m\n",
      "Logistic regression accuracy on shopmania dataset: 69.89%\n",
      "Time taken to process shopmania dataset: 0.01m\n",
      "Logistic regression accuracy on custom dataset: 77.27%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "# Reset dataset to compare\n",
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_lemmatizer(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stopword_removal(df, name, labels):\n",
    "    \n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(remove_stopwords)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    lr_model = get_lr_model(X_train, Y_train)\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "\n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on amazon dataset: 84.00%\n",
      "Time taken to process amazon dataset: 0.01m\n",
      "Logistic regression accuracy on shopmania dataset: 78.23%\n",
      "Time taken to process shopmania dataset: 0.01m\n",
      "Logistic regression accuracy on custom dataset: 79.55%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "# Reset dataset to compare\n",
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_stopword_removal(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon', 84.2, 84.39999999999999, 84.65, 83.25, 84.0]\n",
      "['shopmania', 78.18911685994647, 78.67975022301516, 79.1257805530776, 69.89295272078502, 78.23371989295272]\n",
      "['custom', 79.54545454545455, 81.06060606060606, 78.03030303030303, 77.27272727272727, 79.54545454545455]\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({}, columns=[\"name\", \"remove punctuation\", \"remove numbers\", \"stemmed\", \"lemmatized\", \"remove stopwords\"])\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "    combined = list([key] + dataset[2:])\n",
    "    print(combined)\n",
    "    new_line = pd.DataFrame([combined], columns=[\"name\", \"remove punctuation\", \"remove numbers\", \"stemmed\", \"lemmatized\", \"remove stopwords\"])\n",
    "\n",
    "    results = pd.concat([results, new_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>remove punctuation</th>\n",
       "      <th>remove numbers</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>remove stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon</td>\n",
       "      <td>84.2</td>\n",
       "      <td>84.4</td>\n",
       "      <td>84.65</td>\n",
       "      <td>83.25</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shopmania</td>\n",
       "      <td>78.189117</td>\n",
       "      <td>78.67975</td>\n",
       "      <td>79.125781</td>\n",
       "      <td>69.892953</td>\n",
       "      <td>78.23372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>custom</td>\n",
       "      <td>79.545455</td>\n",
       "      <td>81.060606</td>\n",
       "      <td>78.030303</td>\n",
       "      <td>77.272727</td>\n",
       "      <td>79.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name remove punctuation remove numbers    stemmed lemmatized  \\\n",
       "0     amazon               84.2           84.4      84.65      83.25   \n",
       "0  shopmania          78.189117       78.67975  79.125781  69.892953   \n",
       "0     custom          79.545455      81.060606  78.030303  77.272727   \n",
       "\n",
       "  remove stopwords  \n",
       "0             84.0  \n",
       "0         78.23372  \n",
       "0        79.545455  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(df, name, labels):\n",
    "    \n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(remove_nums)\n",
    "    df['item_name'] = df['item_name'].apply(stemmer_nltk)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = split_dataset(df)\n",
    "\n",
    "    Y_train, Y_test = create_labels(Y_train, Y_test, labels)\n",
    "    X_train, X_test = get_vectors(X_train, X_test)\n",
    "    lr_model = get_lr_model(X_train, Y_train)\n",
    "\n",
    "    lr_acc = get_acc(lr_model, X_test, Y_test)\n",
    "\n",
    "    print(f\"Logistic regression accuracy on {name} dataset: {lr_acc:.2f}%\")\n",
    "\n",
    "    return lr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy on amazon dataset: 85.05%\n",
      "Time taken to process amazon dataset: 0.04m\n",
      "Logistic regression accuracy on shopmania dataset: 78.81%\n",
      "Time taken to process shopmania dataset: 0.02m\n",
      "Logistic regression accuracy on custom dataset: 80.30%\n",
      "Time taken to process custom dataset: 0.00m\n"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "# Reset dataset to compare\n",
    "for key, dataset in datasets.items():\n",
    "    name = key\n",
    "\n",
    "    start = process_time()\n",
    "\n",
    "    lr_acc = run_test(dataset[0], name, dataset[1])\n",
    "\n",
    "    stop = process_time()\n",
    "\n",
    "    print(f\"Time taken to process {name} dataset: {(stop-start)/60:.2f}m\")\n",
    "    datasets[key].append(lr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\University\\FYP\\final\\preprocessing.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University/FYP/final/preprocessing.ipynb#ch0000025?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mitem_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mitem_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(stemmer_nltk)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/University/FYP/final/preprocessing.ipynb#ch0000025?line=12'>13</a>\u001b[0m csv_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcleaned/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m key \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/University/FYP/final/preprocessing.ipynb#ch0000025?line=14'>15</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_csv(csv_name)\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3539'>3540</a>\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3541'>3542</a>\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3542'>3543</a>\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3543'>3544</a>\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3547'>3548</a>\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3548'>3549</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3550'>3551</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3551'>3552</a>\u001b[0m     path_or_buf,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3552'>3553</a>\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3553'>3554</a>\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3554'>3555</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3555'>3556</a>\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3556'>3557</a>\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3557'>3558</a>\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3558'>3559</a>\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3559'>3560</a>\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3560'>3561</a>\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3561'>3562</a>\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3562'>3563</a>\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3563'>3564</a>\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3564'>3565</a>\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3565'>3566</a>\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3566'>3567</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/core/generic.py?line=3567'>3568</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1158'>1159</a>\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1160'>1161</a>\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1161'>1162</a>\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1162'>1163</a>\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1177'>1178</a>\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1178'>1179</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1179'>1180</a>\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1181'>1182</a>\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/format.py?line=1182'>1183</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=236'>237</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=237'>238</a>\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=238'>239</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=239'>240</a>\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=240'>241</a>\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=241'>242</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=242'>243</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=243'>244</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=244'>245</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=245'>246</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=246'>247</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=247'>248</a>\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=248'>249</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=249'>250</a>\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=250'>251</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=251'>252</a>\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=252'>253</a>\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=257'>258</a>\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=258'>259</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/formats/csvs.py?line=260'>261</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\pandas\\io\\common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=694'>695</a>\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=695'>696</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=696'>697</a>\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=698'>699</a>\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=699'>700</a>\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=700'>701</a>\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\grace\\.conda\\envs\\FYP\\lib\\site-packages\\pandas\\io\\common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=568'>569</a>\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=569'>570</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> <a href='file:///c%3A/Users/grace/.conda/envs/FYP/lib/site-packages/pandas/io/common.py?line=570'>571</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'cleaned'"
     ]
    }
   ],
   "source": [
    "reset_data()\n",
    "\n",
    "for key, dataset in datasets.items():\n",
    "\n",
    "    df = dataset[0]\n",
    "\n",
    "    df['item_name'] = df['item_name'].astype(str)\n",
    "    df['item_name'] = df['item_name'].apply(lower)\n",
    "    df['item_name'] = df['item_name'].apply(remove_punc)\n",
    "    df['item_name'] = df['item_name'].apply(remove_nums)\n",
    "    df['item_name'] = df['item_name'].apply(stemmer_nltk)\n",
    "\n",
    "    csv_name = '../cleaned/' + key + '.csv'\n",
    "\n",
    "    df.to_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f206b6c75a13f92bc97dfa1e930151b21df17184be9d9c72a2ae387f01f205b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('FYP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
